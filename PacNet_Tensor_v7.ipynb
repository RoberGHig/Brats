{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'C:\\\\Users\\\\rober\\\\Practicas\\\\training\\\\training_v2.tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_decode(filename_queue):\n",
    "    #Definimos la ruta de los tfrecords\n",
    "    filename = 'C:\\\\Users\\\\rober\\\\Practicas\\\\training\\\\training_v2.tfrecords'\n",
    "    feature={\n",
    "        'image/encoded': tf.FixedLenFeature([], tf.string,default_value=''),\n",
    "        'label/encoded': tf.FixedLenFeature([], tf.string,default_value='')\n",
    "    }\n",
    "    #Función encargada de añadir a una cola FIFO la lista de ficheros, además el parametro epochs\n",
    "    #nos proporciona el numero de tensores que obtendremos en cada iteracción\n",
    "    filename_queue = tf.train.string_input_producer([filename], num_epochs=50)\n",
    "    #Definimos un lector\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(serialized_example, features=feature)\n",
    "    #Diccionario con las características necesarias.Decodificamos el \"record\" leido\n",
    "    #por el lector\n",
    "    #features = tf.parse_single_example(serialized_example,)  \n",
    "    # Nos devuelve la imagen y la etiqueta\n",
    "    #Convierte la etiqueta a su formato original \n",
    "    with tf.name_scope('label'):\n",
    "        label = tf.decode_raw(features['label/encoded'], tf.float32)\n",
    "        label = tf.transpose(tf.reshape(label, [1,240,240]), (0,2,1))\n",
    "    #Convierte la imagen a su formato original \n",
    "    with tf.name_scope('image'):\n",
    "        image = tf.decode_raw(features['image/encoded'], tf.float32)\n",
    "        image = tf.transpose(tf.reshape(image, [4,240,240]), (0,2,1))\n",
    "        #image = tf.reshape(image, [32, 100, 3])\n",
    "    #Realiza el casteo a float32 e int32\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    print(\"image\", image.get_shape(), label.get_shape())\n",
    "    \n",
    "    return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputs(batch_size,num_epochs,filename):\n",
    "    if not num_epochs: num_epochs = None\n",
    "    with tf.name_scope('input'):\n",
    "        filename_queue = tf.train.string_input_producer([filename], num_epochs=num_epochs)\n",
    "        image, label = read_and_decode(filename_queue=filename_queue)\n",
    "        images, sparse_labels = tf.train.shuffle_batch(\n",
    "        [image, label], batch_size=batch_size, num_threads=2,\n",
    "        capacity=15,\n",
    "        min_after_dequeue=10)\n",
    "        return images, sparse_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "def maxpool2d(x, kernel_size=3,strides=1):\n",
    "    return tf.nn.max_pool(x, ksize=[1, kernel_size, kernel_size, 1], \n",
    "                          strides=[1, strides, strides, 1],padding='SAME')\n",
    "def deconv2d(x,W,output_shape,strides=1):\n",
    "    return tf.nn.conv2d_transpose(x,W,output_shape,strides=[1,strides,strides,1],padding='SAME')\n",
    "def print_tensor_shape(tensor,string):\n",
    "    print(string,tensor.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(logits,labels):\n",
    "        labels = tf.to_int64(labels)\n",
    "        print_tensor_shape(logits,\"Logits antes del reshape: \")\n",
    "        print_tensor_shape(labels,\"Labels antes del reshape: \")\n",
    "        #OJO CON EL NUMERO DE CLASES [-1,NUMERO CLASES]\n",
    "        logits_re = tf.reshape(logits,[-1,5])\n",
    "        labels_re = tf.reshape(labels,[-1])\n",
    "        print_tensor_shape(logits_re,\"Logits despues del reshape: \")\n",
    "        print_tensor_shape(labels_re,\"Labels despues del reshape: \")\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_re,logits=logits_re,\n",
    "                                                                       name='cross_entropy')\n",
    "        loss = tf.reduce_mean(cross_entropy,name='cross_entropy_mean')\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(loss,learning_rate,decay_steps,decay_rate):\n",
    "    tf.summary.scalar(loss.op.name,loss)\n",
    "    global_step = tf.Variable(0,name='global_step',trainable=False)\n",
    "    lr = tf.train.exponential_decay(learning_rate,\n",
    "                                   global_step,\n",
    "                                   decay_steps,\n",
    "                                   decay_rate,\n",
    "                                   staircase=True)\n",
    "    tf.summary.scalar('learning_rate',lr)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "    train_op=optimizer.minimize(loss,global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    #PESOS DEL PATH SMALL\n",
    "    # 3x3 conv, 4 input, 64 outputs\n",
    "    'w_s1': tf.Variable(tf.random_normal([3, 3, 4, 64])),\n",
    "    # 3x3 conv, 64 inputs, 64 outputs\n",
    "    'w_s2': tf.Variable(tf.random_normal([3, 3, 64, 64])),\n",
    "    # 3x3 conv, 64 inputs, 64 outputs\n",
    "    'w_s3': tf.Variable(tf.random_normal([3, 3, 64, 64])),\n",
    "    # 3x3 conv, 64 inputs, 64 outputs\n",
    "    'w_s4': tf.Variable(tf.random_normal([3, 3, 64, 64])),\n",
    "    # 3x3 conv, 64 inputs, 64 outputs\n",
    "    'w_s5': tf.Variable(tf.random_normal([3, 3, 64, 64])),\n",
    "    # 3x3 conv, 4 input, 64 outputs\n",
    "    'w_s6': tf.Variable(tf.random_normal([3, 3, 4, 64])),\n",
    "    # 3x3 conv, 64 inputs, 64 outputs\n",
    "    'w_s7': tf.Variable(tf.random_normal([3, 3, 64, 64])),\n",
    "    # 3x3 conv, 64 inputs, 64 outputs\n",
    "    'w_s8': tf.Variable(tf.random_normal([3, 3, 64, 64])),\n",
    "    # 3x3 conv, 64 inputs, 64 outputs\n",
    "    'w_s9': tf.Variable(tf.random_normal([3, 3, 64, 64])),\n",
    "    \n",
    "    #PESOS DEL PATH MEDIUM\n",
    "    # 7x7 conv, 4 input, 128 outputs\n",
    "    'w_m1': tf.Variable(tf.random_normal([7, 7, 4, 128])),\n",
    "    # 7x7 conv, 128 inputs, 128 outputs\n",
    "    'w_m2': tf.Variable(tf.random_normal([7, 7, 128, 128])),\n",
    "    # 7x7 conv, 128 inputs, 128 outputs\n",
    "    'w_m3': tf.Variable(tf.random_normal([7, 7, 128, 128])),\n",
    "    # 7x7 conv, 128 inputs, 128 outputs\n",
    "    'w_m4': tf.Variable(tf.random_normal([7, 7, 128, 128])),\n",
    "    # 7x7 conv, 128 inputs, 128 outputs\n",
    "    'w_m5': tf.Variable(tf.random_normal([7, 7, 128, 128])),\n",
    "    # 7x7 conv, 4 input, 128 outputs\n",
    "    'w_m6': tf.Variable(tf.random_normal([7, 7, 4, 128])),\n",
    "    # 7x7 conv, 128 inputs, 128 outputs\n",
    "    'w_m7': tf.Variable(tf.random_normal([7, 7, 128, 128])),\n",
    "    # 7x7 conv, 128 inputs, 128 outputs\n",
    "    'w_m8': tf.Variable(tf.random_normal([7, 7, 128, 128])),\n",
    "    # 7x7 conv, 128 inputs, 128 outputs\n",
    "    'w_m9': tf.Variable(tf.random_normal([7, 7, 128, 128])),\n",
    "    \n",
    "    #PESOS DEL PATH LARGE\n",
    "    # 11x11 conv, 4 input, 256 outputs\n",
    "    'w_l1': tf.Variable(tf.random_normal([11,11, 4, 256])),\n",
    "    # 11x11 conv, 256 inputs, 256 outputs\n",
    "    'w_l2': tf.Variable(tf.random_normal([11, 11, 256, 256])),\n",
    "    # 11x11 conv, 256 inputs, 256 outputs\n",
    "    'w_l3': tf.Variable(tf.random_normal([11, 11, 256, 256])),\n",
    "    # 11x11 conv, 256 inputs, 256 outputs\n",
    "    'w_l4': tf.Variable(tf.random_normal([11, 11, 256, 256])),\n",
    "    # 11x11 conv, 256 inputs, 256 outputs\n",
    "    'w_l5': tf.Variable(tf.random_normal([11, 11, 256, 256])),\n",
    "    # 11x11 conv, 4 input, 256 outputs\n",
    "    'w_l6': tf.Variable(tf.random_normal([11,11, 4, 256])),\n",
    "    # 11x11 conv, 256 inputs, 256 outputs\n",
    "    'w_l7': tf.Variable(tf.random_normal([11, 11, 256, 256])),\n",
    "    # 11x11 conv, 256 inputs, 256 outputs\n",
    "    'w_l8': tf.Variable(tf.random_normal([11, 11, 256, 256])),\n",
    "    # 11x11 conv, 256 inputs, 256 outputs\n",
    "    'w_l9': tf.Variable(tf.random_normal([11, 11, 256, 256])),\n",
    "    \n",
    "    #PESOS DEL PATH FUSION\n",
    "    # 1x1 conv, 448 inputs, 300 outputs\n",
    "    'w_f1': tf.Variable(tf.random_normal([1, 1, 448, 300])),\n",
    "    # 1x1 conv, 300 inputs, 512 outputs\n",
    "    'w_f2': tf.Variable(tf.random_normal([1, 1, 300, 512])),\n",
    "    # 1x1 conv, 512 inputs, 5 outputs\n",
    "    'w_f3': tf.Variable(tf.random_normal([1, 1, 512, 5])),\n",
    "    # 3x3 conv, 448 inputs, 300 outputs\n",
    "    'w_f4': tf.Variable(tf.random_normal([3, 3, 448, 300])),\n",
    "    # 1x1 conv, 300 inputs, 512 outputs\n",
    "    'w_f5': tf.Variable(tf.random_normal([1, 1, 300, 512])),\n",
    "    # 1x1 conv, 512 inputs, 5 outputs\n",
    "    'w_f6': tf.Variable(tf.random_normal([1, 1, 512, 5])),\n",
    "    \n",
    "    #PESOS DEL PATH DECONVOLUCIONAL\n",
    "    # 1x1 conv, 5 inputs, 5 outputs\n",
    "    'w_fdec1': tf.Variable(tf.random_normal([1, 1, 5, 5])),\n",
    "    # 1x1 conv, 5 inputs, 5 outputs\n",
    "    'w_fdec2': tf.Variable(tf.random_normal([1, 1, 5, 5])),\n",
    "    \n",
    "    #PESOS DEL PATH FINAL FUSION\n",
    "    # 1x1 conv, 10 inputs, 5 outputs\n",
    "    'w_final_fusion': tf.Variable(tf.random_normal([1, 1, 10, 5])),\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "biases = {\n",
    "    #BIASES DEL PATH SMALL\n",
    "    'b_s_1': tf.Variable(tf.random_normal([64])),\n",
    "    'b_s_2': tf.Variable(tf.random_normal([64])),\n",
    "    'b_s_3': tf.Variable(tf.random_normal([64])),\n",
    "    'b_s_4': tf.Variable(tf.random_normal([64])),\n",
    "    'b_s_5': tf.Variable(tf.random_normal([64])),\n",
    "    'b_s_6': tf.Variable(tf.random_normal([64])),\n",
    "    'b_s_7': tf.Variable(tf.random_normal([64])),\n",
    "    'b_s_8': tf.Variable(tf.random_normal([64])),\n",
    "    'b_s_9': tf.Variable(tf.random_normal([64])),\n",
    "    \n",
    "    #BIASES DEL PATH MEDIUM\n",
    "    'b_m_1': tf.Variable(tf.random_normal([128])),\n",
    "    'b_m_2': tf.Variable(tf.random_normal([128])),\n",
    "    'b_m_3': tf.Variable(tf.random_normal([128])),\n",
    "    'b_m_4': tf.Variable(tf.random_normal([128])),\n",
    "    'b_m_5': tf.Variable(tf.random_normal([128])),\n",
    "    'b_m_6': tf.Variable(tf.random_normal([128])),\n",
    "    'b_m_7': tf.Variable(tf.random_normal([128])),\n",
    "    'b_m_8': tf.Variable(tf.random_normal([128])),\n",
    "    'b_m_9': tf.Variable(tf.random_normal([128])),\n",
    "    \n",
    "    #BIASES DEL PATH LARGE\n",
    "    'b_l_1': tf.Variable(tf.random_normal([256])),\n",
    "    'b_l_2': tf.Variable(tf.random_normal([256])),\n",
    "    'b_l_3': tf.Variable(tf.random_normal([256])),\n",
    "    'b_l_4': tf.Variable(tf.random_normal([256])),\n",
    "    'b_l_5': tf.Variable(tf.random_normal([256])),\n",
    "    'b_l_6': tf.Variable(tf.random_normal([256])),\n",
    "    'b_l_7': tf.Variable(tf.random_normal([256])),\n",
    "    'b_l_8': tf.Variable(tf.random_normal([256])),\n",
    "    'b_l_9': tf.Variable(tf.random_normal([256])),\n",
    "    \n",
    "    #BIASES DEL PATH FUSION\n",
    "    'b_f_1': tf.Variable(tf.random_normal([300])),\n",
    "    'b_f_2': tf.Variable(tf.random_normal([512])),\n",
    "    'b_f_3': tf.Variable(tf.random_normal([5])),\n",
    "    'b_f_4': tf.Variable(tf.random_normal([300])),\n",
    "    'b_f_5': tf.Variable(tf.random_normal([512])),\n",
    "    'b_f_6': tf.Variable(tf.random_normal([5])),\n",
    "    \n",
    "    #BIASES DEL PATH FINAL FUSION\n",
    "    'b_ff': tf.Variable(tf.random_normal([5]))\n",
    "}\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(x,weights,biases,dropout):\n",
    "    print_tensor_shape(x,'X antes del Reshaping')\n",
    "    #x = tf.reshape(x, shape=[-1, 240, 240, 4])\n",
    "    x = tf.reshape(x,[batch_size,240,240,-1])\n",
    "    print_tensor_shape(x,'X despues del Reshaping')\n",
    "    print(\"\\n\")\n",
    "    #WHERE\n",
    "    #SMALL\n",
    "    conv1 = conv2d(x,weights['w_s1'],biases['b_s_1'],strides=1)\n",
    "    print_tensor_shape(conv1,'Shape conv1')\n",
    "    pool1 = maxpool2d(conv1,kernel_size=3,strides=1)\n",
    "    print_tensor_shape(pool1,'Shape pool1')\n",
    "    conv2 = conv2d(pool1,weights['w_s2'],biases['b_s_2'],strides=1)\n",
    "    print_tensor_shape(conv2,'Shape conv2')\n",
    "    pool2 = maxpool2d(conv2,kernel_size=3,strides=1)\n",
    "    print_tensor_shape(pool2,'Shape pool2')\n",
    "    conv3 = conv2d(pool2,weights['w_s3'],biases['b_s_3'],strides=1)\n",
    "    print_tensor_shape(conv3,'Shape conv3')\n",
    "    pool3 = maxpool2d(conv3,kernel_size=3,strides=1)\n",
    "    print_tensor_shape(pool3,'Shape pool3')\n",
    "    conv4 = conv2d(pool3,weights['w_s4'],biases['b_s_4'],strides=1)\n",
    "    print_tensor_shape(conv4,'Shape conv4')\n",
    "    pool4 = maxpool2d(conv4,kernel_size=3,strides=1)\n",
    "    print_tensor_shape(pool4,'Shape pool4')\n",
    "    conv5 = conv2d(pool4,weights['w_s5'],biases['b_s_5'],strides=1)\n",
    "    print_tensor_shape(conv5,'Shape conv5')\n",
    "    pool5 = maxpool2d(conv5,kernel_size=3,strides=1)\n",
    "    print_tensor_shape(pool5,'Shape pool5')\n",
    "    print(\"\\n\")\n",
    "    #MEDIUM\n",
    "    conv6 = conv2d(x,weights['w_m1'],biases['b_m_1'],strides=1)\n",
    "    print_tensor_shape(conv6,'Shape conv6')\n",
    "    pool6 = maxpool2d(conv6,kernel_size=3,strides=1)\n",
    "    print_tensor_shape(pool6,'Shape pool6')\n",
    "    conv7 = conv2d(pool6,weights['w_m2'],biases['b_m_2'],strides=1)\n",
    "    print_tensor_shape(conv7,'Shape conv7')\n",
    "    pool7 = maxpool2d(conv7,kernel_size=3,strides=1)\n",
    "    print_tensor_shape(pool7,'Shape pool7')\n",
    "    conv8 = conv2d(pool7,weights['w_m3'],biases['b_m_3'],strides=1)\n",
    "    print_tensor_shape(conv8,'Shape conv8')\n",
    "    pool8 = maxpool2d(conv8,kernel_size=3,strides=1)\n",
    "    print_tensor_shape(pool8,'Shape pool8')\n",
    "    conv9 = conv2d(pool8,weights['w_m4'],biases['b_m_4'],strides=1)\n",
    "    print_tensor_shape(conv9,'Shape conv9')\n",
    "    pool9 = maxpool2d(conv9,kernel_size=3,strides=1)\n",
    "    print_tensor_shape(pool9,'Shape pool9')\n",
    "    conv10 = conv2d(pool9,weights['w_m5'],biases['b_m_5'],strides=1)\n",
    "    print_tensor_shape(conv10,'Shape conv10')\n",
    "    pool10 = maxpool2d(conv10,kernel_size=3,strides=1)\n",
    "    print_tensor_shape(pool10,'Shape pool10')\n",
    "    print(\"\\n\")\n",
    "    #LARGE\n",
    "    conv11 = conv2d(x,weights['w_l1'],biases['b_l_1'],strides=1)\n",
    "    print_tensor_shape(conv11,'Shape conv11')\n",
    "    pool11 = maxpool2d(conv11,kernel_size=3,strides=1)\n",
    "    print_tensor_shape(pool11,'Shape pool11')\n",
    "    conv12 = conv2d(pool11,weights['w_l2'],biases['b_l_2'],strides=1)\n",
    "    print_tensor_shape(conv12,'Shape conv12')\n",
    "    pool12 = maxpool2d(conv12,kernel_size=3,strides=1)\n",
    "    print_tensor_shape(pool12,'Shape pool12')\n",
    "    conv13 = conv2d(pool12,weights['w_l3'],biases['b_l_3'],strides=1)\n",
    "    print_tensor_shape(conv13,'Shape conv13')\n",
    "    pool13 = maxpool2d(conv13,kernel_size=3,strides=1)\n",
    "    print_tensor_shape(pool11,'Shape pool13')\n",
    "    conv14 = conv2d(pool13,weights['w_l4'],biases['b_l_4'],strides=1)\n",
    "    print_tensor_shape(conv12,'Shape conv14')\n",
    "    pool14 = maxpool2d(conv14,kernel_size=3,strides=1)\n",
    "    print_tensor_shape(pool14,'Shape pool14')\n",
    "    conv15 = conv2d(pool14,weights['w_l5'],biases['b_l_5'],strides=1)\n",
    "    print_tensor_shape(conv15,'Shape conv15')\n",
    "    pool15 = maxpool2d(conv15,kernel_size=3,strides=1)\n",
    "    print_tensor_shape(pool14,'Shape pool15')\n",
    "    print(\"\\n\")\n",
    "    #FUSION\n",
    "    conv16 = tf.concat((pool5,pool10,pool15),3)\n",
    "    print_tensor_shape(conv16,'Shape conv16_concat')\n",
    "    conv16 = conv2d(conv16,weights['w_f1'],biases['b_f_1'],strides=1)\n",
    "    print_tensor_shape(conv16,'Shape conv16')\n",
    "    drop1 = tf.nn.dropout(conv16,dropout)\n",
    "    print_tensor_shape(drop1,'Shape drop1')\n",
    "    conv17 = conv2d(drop1,weights['w_f2'],biases['b_f_2'],strides=1)\n",
    "    print_tensor_shape(conv17,'Shape conv17')\n",
    "    drop2 = tf.nn.dropout(conv17,dropout)\n",
    "    print_tensor_shape(drop2,'Shape drop2')\n",
    "    conv18 = conv2d(drop2,weights['w_f3'],biases['b_f_3'],strides=1)\n",
    "    print_tensor_shape(conv18,'Shape conv18')\n",
    "    print(\"\\n\")\n",
    "    #WHAT\n",
    "    #LARGE\n",
    "    conv1l =conv2d(x,weights['w_l6'],biases['b_l_6'],strides=1)\n",
    "    print_tensor_shape(conv1l,'Shape conv1l')\n",
    "    pool1l= maxpool2d(conv1l,kernel_size=2,strides=2)\n",
    "    print_tensor_shape(pool1l,'Shape pool1l')\n",
    "    conv2l =conv2d(pool1l,weights['w_l7'],biases['b_l_7'],strides=1)\n",
    "    print_tensor_shape(conv2l,'Shape conv2l')\n",
    "    pool2l= maxpool2d(conv2l,kernel_size=2,strides=2)\n",
    "    print_tensor_shape(pool2l,'Shape pool2l')\n",
    "    conv3l =conv2d(pool2l,weights['w_l8'],biases['b_l_8'],strides=1)\n",
    "    print_tensor_shape(conv3l,'Shape conv3l')\n",
    "    pool3l= maxpool2d(conv3l,kernel_size=2,strides=2)\n",
    "    print_tensor_shape(pool3l,'Shape pool3l')\n",
    "    conv4l =conv2d(pool3l,weights['w_l9'],biases['b_l_9'],strides=1)\n",
    "    print_tensor_shape(conv4l,'Shape conv4l')\n",
    "    print(\"\\n\")\n",
    "    #MEDIUM\n",
    "    conv1m =conv2d(x,weights['w_m6'],biases['b_m_6'],strides=1)\n",
    "    print_tensor_shape(conv1m,'Shape conv1m')\n",
    "    pool1m= maxpool2d(conv1m,kernel_size=2,strides=2)\n",
    "    print_tensor_shape(pool1m,'Shape pool1m')\n",
    "    conv2m = conv2d(pool1m,weights['w_m7'],biases['b_m_7'],strides=1)\n",
    "    print_tensor_shape(conv2m,'Shape conv2m')\n",
    "    pool2m= maxpool2d(conv2m,kernel_size=2,strides=2)\n",
    "    print_tensor_shape(pool2m,'Shape pool2m')\n",
    "    conv3m = conv2d(pool2m,weights['w_m8'],biases['b_m_8'],strides=1)\n",
    "    print_tensor_shape(conv3m,'Shape conv3m')\n",
    "    pool3m= maxpool2d(conv3m,kernel_size=2,strides=2)\n",
    "    print_tensor_shape(pool3m,'Shape pool3m')\n",
    "    conv4m = conv2d(pool3m,weights['w_m9'],biases['b_m_9'],strides=1)\n",
    "    print_tensor_shape(conv4m,'Shape conv4m')\n",
    "    print(\"\\n\")\n",
    "    #SMALL\n",
    "    conv1s =conv2d(x,weights['w_s6'],biases['b_s_6'],strides=1)\n",
    "    print_tensor_shape(conv1s,'Shape conv1s')\n",
    "    pool1s= maxpool2d(conv1s,kernel_size=2,strides=2)\n",
    "    print_tensor_shape(pool1s,'Shape pool1s')\n",
    "    conv2s = conv2d(pool1s,weights['w_s7'],biases['b_s_7'],strides=1)\n",
    "    print_tensor_shape(conv2s,'Shape conv2s')\n",
    "    pool2s= maxpool2d(conv2s,kernel_size=2,strides=2)\n",
    "    print_tensor_shape(pool2s,'Shape pool2s')\n",
    "    conv3s = conv2d(pool2s,weights['w_s8'],biases['b_s_8'],strides=1)\n",
    "    print_tensor_shape(conv3s,'Shape conv3s')\n",
    "    pool3s= maxpool2d(conv3s,kernel_size=2,strides=2)\n",
    "    print_tensor_shape(pool3s,'Shape pool3s')\n",
    "    conv4s = conv2d(pool3s,weights['w_s9'],biases['b_s_9'],strides=1)\n",
    "    print_tensor_shape(conv4s,'Shape conv4s')\n",
    "    print(\"\\n\")\n",
    "    #FUSION\n",
    "    convf1 = tf.concat((conv4l,conv4m,conv4s),3) \n",
    "    print_tensor_shape(convf1,'Shape convf1')\n",
    "    convf2 = conv2d(convf1,weights['w_f4'],biases['b_f_4'],strides=1)\n",
    "    print_tensor_shape(convf2,'Shape convf2')\n",
    "    poolf2= maxpool2d(convf2,kernel_size=2,strides=2)\n",
    "    print_tensor_shape(poolf2,'Shape poolf2')\n",
    "    dropf3 = tf.nn.dropout(poolf2,dropout)\n",
    "    print_tensor_shape(dropf3,'Shape dropf3')\n",
    "    convf4 = conv2d(dropf3,weights['w_f5'],biases['b_f_5'],strides=1)\n",
    "    print_tensor_shape(convf4,'Shape convf4')\n",
    "    dropf5 = tf.nn.dropout(convf4,dropout)\n",
    "    print_tensor_shape(dropf5,'Shape dropf5')\n",
    "    convf5 = conv2d(dropf5,weights['w_f6'],biases['b_f_6'],strides=1)\n",
    "    print_tensor_shape(convf5,'Shape convf5')\n",
    "    print(\"\\n\")\n",
    "    #DECONV\n",
    "    deconv1= deconv2d(convf5,weights['w_fdec1'],output_shape = [batch_size,60,60,5],strides=4)\n",
    "    print_tensor_shape(deconv1,'Shape deconv1')\n",
    "    deconv2 = deconv2d(deconv1,weights['w_fdec2'],output_shape = [batch_size,240,240,5],strides=4)\n",
    "    print_tensor_shape(deconv2,'Shape deconv2')\n",
    "    print(\"\\n\")\n",
    "    #FINAL FUSION\n",
    "    conv_final_fusion1 = tf.concat((conv18,deconv2),3)\n",
    "    print_tensor_shape(conv_final_fusion1,'Shape conv_final_fusion1')\n",
    "    conv_final_fusion2 = conv2d(conv_final_fusion1,weights['w_final_fusion'],biases['b_ff'],strides=1)\n",
    "    print_tensor_shape(conv_final_fusion2,'Shape conv_final_fusion2')\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return conv_final_fusion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "decay_rate = 1.0\n",
    "decay_steps = 1000\n",
    "num_epochs = 1\n",
    "batch_size = 1\n",
    "num_input = 57600\n",
    "filename = 'C:\\\\Users\\\\rober\\\\Practicas\\\\training\\\\training_v2.tfrecords'\n",
    "checkpoint = 'C:\\\\Users\\\\rober\\\\Practicas\\\\training\\\\checkpoint'\n",
    "X = tf.placeholder(tf.float32, [None, num_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    images,labels = inputs(batch_size,num_epochs,filename)\n",
    "    print_tensor_shape(images,'Images Despues de la funcion inputs')\n",
    "    print_tensor_shape(labels,'Labels Despues de la funcion inputs')\n",
    "    #results = inference(images)\n",
    "    results = net(images,weights,biases,dropout)\n",
    "    perdidas = loss(results,labels)\n",
    "    train_op = training(perdidas,learning_rate,decay_steps,decay_rate)\n",
    "    summary_op =tf.summary.merge_all()\n",
    "    init_op = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    sess =tf.Session()\n",
    "    summary_writer = tf.summary.FileWriter(checkpoint,sess.graph)\n",
    "    sess.run(init_op)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    '''try:\n",
    "        step = 0\n",
    "        while not coord.should_stop():\n",
    "            start_time = time.time()\n",
    "            _,loss_value = sess.run([train_op,perdidas])\n",
    "            duration = time.time()-start_time\n",
    "            if step % 100 == 0:\n",
    "                print('OUTPUT:Step %d: loss = %.3f (%.3f sec)' % (step,loss_value,duration))\n",
    "                summary_str = sess.run(summary_op)\n",
    "                summary_writer.add_summary(summary_str,step)\n",
    "                summary_writer.flush()\n",
    "            if step % 1000 == 0:\n",
    "                checkpoint_path = os.path.join(checkpoint,'model.ckpt')\n",
    "                saver.save(sess,checkpoint_path,global_step = step)\n",
    "            step +=1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('OUTPUT: Done training for %d epochs, %d steps',num_epochs,step)\n",
    "        checkpoint_path = os.path.join(checkpoint,'model.ckpt')\n",
    "        saver.save(sess,checkpoint_path,global_step = step)\n",
    "    #finally:'''\n",
    "        coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image (4, 240, 240) (1, 240, 240)\n",
      "Images Despues de la funcion inputs (1, 4, 240, 240)\n",
      "Labels Despues de la funcion inputs (1, 1, 240, 240)\n",
      "X antes del Reshaping (1, 4, 240, 240)\n",
      "X despues del Reshaping (1, 240, 240, 4)\n",
      "\n",
      "\n",
      "Shape conv1 (1, 240, 240, 64)\n",
      "Shape pool1 (1, 240, 240, 64)\n",
      "Shape conv2 (1, 240, 240, 64)\n",
      "Shape pool2 (1, 240, 240, 64)\n",
      "Shape conv3 (1, 240, 240, 64)\n",
      "Shape pool3 (1, 240, 240, 64)\n",
      "Shape conv4 (1, 240, 240, 64)\n",
      "Shape pool4 (1, 240, 240, 64)\n",
      "Shape conv5 (1, 240, 240, 64)\n",
      "Shape pool5 (1, 240, 240, 64)\n",
      "\n",
      "\n",
      "Shape conv6 (1, 240, 240, 128)\n",
      "Shape pool6 (1, 240, 240, 128)\n",
      "Shape conv7 (1, 240, 240, 128)\n",
      "Shape pool7 (1, 240, 240, 128)\n",
      "Shape conv8 (1, 240, 240, 128)\n",
      "Shape pool8 (1, 240, 240, 128)\n",
      "Shape conv9 (1, 240, 240, 128)\n",
      "Shape pool9 (1, 240, 240, 128)\n",
      "Shape conv10 (1, 240, 240, 128)\n",
      "Shape pool10 (1, 240, 240, 128)\n",
      "\n",
      "\n",
      "Shape conv11 (1, 240, 240, 256)\n",
      "Shape pool11 (1, 240, 240, 256)\n",
      "Shape conv12 (1, 240, 240, 256)\n",
      "Shape pool12 (1, 240, 240, 256)\n",
      "Shape conv13 (1, 240, 240, 256)\n",
      "Shape pool13 (1, 240, 240, 256)\n",
      "Shape conv14 (1, 240, 240, 256)\n",
      "Shape pool14 (1, 240, 240, 256)\n",
      "Shape conv15 (1, 240, 240, 256)\n",
      "Shape pool15 (1, 240, 240, 256)\n",
      "\n",
      "\n",
      "Shape conv16_concat (1, 240, 240, 448)\n",
      "Shape conv16 (1, 240, 240, 300)\n",
      "Shape drop1 (1, 240, 240, 300)\n",
      "Shape conv17 (1, 240, 240, 512)\n",
      "Shape drop2 (1, 240, 240, 512)\n",
      "Shape conv18 (1, 240, 240, 5)\n",
      "\n",
      "\n",
      "Shape conv1l (1, 240, 240, 256)\n",
      "Shape pool1l (1, 120, 120, 256)\n",
      "Shape conv2l (1, 120, 120, 256)\n",
      "Shape pool2l (1, 60, 60, 256)\n",
      "Shape conv3l (1, 60, 60, 256)\n",
      "Shape pool3l (1, 30, 30, 256)\n",
      "Shape conv4l (1, 30, 30, 256)\n",
      "\n",
      "\n",
      "Shape conv1m (1, 240, 240, 128)\n",
      "Shape pool1m (1, 120, 120, 128)\n",
      "Shape conv2m (1, 120, 120, 128)\n",
      "Shape pool2m (1, 60, 60, 128)\n",
      "Shape conv3m (1, 60, 60, 128)\n",
      "Shape pool3m (1, 30, 30, 128)\n",
      "Shape conv4m (1, 30, 30, 128)\n",
      "\n",
      "\n",
      "Shape conv1s (1, 240, 240, 64)\n",
      "Shape pool1s (1, 120, 120, 64)\n",
      "Shape conv2s (1, 120, 120, 64)\n",
      "Shape pool2s (1, 60, 60, 64)\n",
      "Shape conv3s (1, 60, 60, 64)\n",
      "Shape pool3s (1, 30, 30, 64)\n",
      "Shape conv4s (1, 30, 30, 64)\n",
      "\n",
      "\n",
      "Shape convf1 (1, 30, 30, 448)\n",
      "Shape convf2 (1, 30, 30, 300)\n",
      "Shape poolf2 (1, 15, 15, 300)\n",
      "Shape dropf3 (1, 15, 15, 300)\n",
      "Shape convf4 (1, 15, 15, 512)\n",
      "Shape dropf5 (1, 15, 15, 512)\n",
      "Shape convf5 (1, 15, 15, 5)\n",
      "\n",
      "\n",
      "Shape deconv1 (1, 60, 60, 5)\n",
      "Shape deconv2 (1, 240, 240, 5)\n",
      "\n",
      "\n",
      "Shape conv_final_fusion1 (1, 240, 240, 10)\n",
      "Shape conv_final_fusion2 (1, 240, 240, 5)\n",
      "\n",
      "\n",
      "Logits antes del reshape:  (1, 240, 240, 5)\n",
      "Labels antes del reshape:  (1, 1, 240, 240)\n",
      "Logits despues del reshape:  (57600, 5)\n",
      "Labels despues del reshape:  (57600,)\n"
     ]
    }
   ],
   "source": [
    "main(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
